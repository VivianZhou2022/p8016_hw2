---
title: "p8106_hw2_qz2266"
author: "Qing Zhou"
date: "2023-03-10"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(splines)
library(mgcv)
library(earth)
library(pdp)
library(ggplot2)
library(gridExtra)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Data preparation

In this exercise, we build nonlinear models using the “College” data. The dataset
contains statistics for 565 US Colleges from a previous issue of US News and World Report.
The response variable is the out-of-state tuition (Outstate). 

```{r}
college_df = read_csv('./data/College.csv') %>% 
  janitor::clean_names() %>%
  na.omit() %>%
  relocate("outstate", .after = "grad_rate") %>%
  select(-college)
```

We then partition the dataset into two parts: training data (80%) and test data (20%)

```{r split}
set.seed(1)
trainRows <- createDataPartition(y = college_df$outstate, p = 0.8, list = FALSE)

# Training data
train_df = college_df[trainRows, ]
x_train = model.matrix(outstate~.,train_df)[, -1]
y_train = train_df$outstate
# Testing data
test_df = college_df[-trainRows, ]
x_test = model.matrix(outstate~.,test_df)[, -1]
y_test = test_df$outstate
```

# Exploratory data analysis

```{r EDA, message=FALSE, fig.width = 10, fig.height = 10, out.width="90%"}
theme1 = trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(.8, .1, .1, 1)
theme1$plot.line$lwd = 2
theme1$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

# feature plot
featurePlot(x_train, y_train, plot = "scatter", labels = c("","Y"), type = c("p"), layout = c(4, 4))
```

- Based on the feature plot, we can see there might be linear trends between the outcome and predictors `perc_alumni`, `grad_rate`, `ph_d`, `terminal`, `top25perc`, `room_board`, and `top10perc`.


# a). Smoothing spline models

Fit smoothing spline models using perc.alumni as the only predictor of Outstate for a range of degrees of freedom.

*1). The degree of freedom is obtained by generalized cross-validation:*

```{r}
set.seed(1)

# fit model
fit.ss = smooth.spline(train_df$perc_alumni, train_df$outstate)
# optimal degrees of freedom based on cross-validation
fit.ss$df

#plot the resulting fits
palumnilims <- range(train_df$perc_alumni)
palumni.grid <- seq(from = palumnilims[1],to = palumnilims[2])

pred.ss = predict(fit.ss, x = palumni.grid)
pred.ss.df = data.frame(pred = pred.ss$y, perc_alumni = palumni.grid)

p = ggplot(data = test_df, aes(x = perc_alumni, y = outstate)) +
geom_point(color = rgb(.2, .4, .2, .5))

p + geom_line(aes(x = perc_alumni, y = pred), data = pred.ss.df, color = rgb(.8, .1, .1, 1)) + theme_bw()
```

*2). The degree of freedom is obtained by my choice:*

```{r}
set.seed(1)

# 2 degree of freedom
fit.ss = smooth.spline(train_df$perc_alumni, train_df$outstate, df = 2) 
fit.ss$df 

pred.ss = predict(fit.ss, x = palumni.grid)
pred.ss.df = data.frame(pred = pred.ss$y, perc_alumni = palumni.grid)

p = ggplot(data = test_df, aes(x = perc_alumni, y = outstate)) +
geom_point(color = rgb(.2, .4, .2, .5))

p + geom_line(aes(x = perc_alumni, y = pred), data = pred.ss.df, color = rgb(.8, .1, .1, 1)) + theme_bw()


# 20 degrees of freedom
fit.ss = smooth.spline(train_df$perc_alumni, train_df$outstate, df = 20) 
fit.ss$df 

pred.ss = predict(fit.ss, x = palumni.grid)
pred.ss.df = data.frame(pred = pred.ss$y, perc_alumni = palumni.grid)

p = ggplot(data = test_df, aes(x = perc_alumni, y = outstate)) +
geom_point(color = rgb(.2, .4, .2, .5))

p + geom_line(aes(x = perc_alumni, y = pred), data = pred.ss.df, color = rgb(.8, .1, .1, 1)) + theme_bw()

```

- When setting df = 2 and df = 20, we can see that df = 2 shows a more linear trend, while df = 10 shows a more wiggly fitted line. Therefore, larger df values make the line much more wiggly, suggesting potential overfitting issue, while smaller degrees of freedom make the fitted line more linear and smooth, but less flexible and thus might underfit the data points.

- The degree of freedom obtained by generalized cross validation is 3.779231, and the fitted curve has more details than the line of df = 2 and is more linear and smooth than the line of df = 10. By comparing these three models, we conclude that the optimized model fits the data best, with a positive relationship between `perc_alumni` and `Outstate`.


# b). Generalized additive models

```{r}
set.seed(1)

# 10-fold cross-validation  
ctrl1 = trainControl(method = "cv", number = 10)

# fit a gam model using all predictors
gam_fit_all = train(x_train, y_train, 
                 method = "gam",
                 trControl = ctrl1) 

gam_fit_all$bestTune
gam_fit_all$finalModel

        
# fit GAM alternatively
gam_fit_alt <- train(x_train, y_train, 
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE)),
                 trControl = ctrl1) 

gam_fit_alt$bestTune       
gam_fit_alt$finalModel

```     
1. The GAM model for all predictors
.outcome ~ s(perc_alumni) + s(terminal) + s(books) + s(grad_rate) + 
    s(ph_d) + s(top10perc) + s(top25perc) + s(s_f_ratio) + s(personal) + 
    s(p_undergrad) + s(room_board) + s(enroll) + s(accept) + 
    s(f_undergrad) + s(apps) + s(expend)

Estimated degrees of freedom:
6.05 1.00 2.17 3.56 1.81 1.00 1.00 
3.69 1.00 1.00 2.47 1.00 4.19 5.51 
4.45 6.87  total = 47.75 

GCV score: 2824207   

2.The GAM model for the selection specification:
.outcome ~ s(perc_alumni) + s(terminal) + s(books) + s(grad_rate) + 
    s(ph_d) + s(top10perc) + s(top25perc) + s(s_f_ratio) + s(personal) + 
    s(p_undergrad) + s(room_board) + s(enroll) + s(accept) + 
    s(f_undergrad) + s(apps) + s(expend)

Estimated degrees of freedom:
6.077 0.198 1.095 1.437 0.000 0.832 0.000 
3.853 0.638 0.796 3.770 1.000 4.625 5.917 
4.603 5.930  total = 41.77 

GCV score: 2766492    


- From the result above, we remove `PhD`and `Top25perc` since their df = 0. Therefore, my final GAM model has 14 predictors. It doesn't have all 16 predictors. The model is as follows:

- Outstate ~ s(perc.alumni) + s(Terminal) + s(Books) + s(Grad.Rate) + 
  s(Top10perc) + s(S.F.Ratio) + s(Personal) +  s(P.Undergrad) + s(Room.Board) + 
  s(Enroll) + s(Accept) + s(F.Undergrad) + s(Apps) + s(Expend)

        
*Plot of final model*

```{r fig.height=10, fig.width=10}
gam_final <- gam(outstate ~ s(perc_alumni) + s(terminal) + s(books) + 
                s(grad_rate)+ s(top10perc) + s(s_f_ratio) + s(personal) + 
                s(p_undergrad) + s(room_board) + s(enroll) + s(accept) +
                s(f_undergrad) + s(apps) + s(expend), 
              data = train_df) 

plot(gam_final)
```

*Report test error.*

```{r}
# Calculate training MSE of GAM model 1
gam_train_MSE = mean((y_train - predict(gam_fit_all))^2)
gam_train_MSE
gam_train_RMSE = sqrt(gam_train_MSE)
gam_train_RMSE

# Calculate test MSE of GAM model 1
test_predictions = predict(gam_fit_all, x_test)
gam_test_MSE = mean((y_test - test_predictions)^2)
gam_test_MSE
gam_test_RMSE = sqrt(gam_test_MSE)
gam_test_RMSE
```

```{r}
# Calculate training MSE of GAM model 2
gam_train_MSE = mean((y_train - predict(gam_fit_alt))^2)
gam_train_MSE
gam_train_RMSE = sqrt(gam_train_MSE)
gam_train_RMSE

# Calculate test MSE of GAM model 2
test_predictions = predict(gam_fit_alt, x_test)
gam_test_MSE = mean((y_test - test_predictions)^2)
gam_test_MSE
gam_test_RMSE = sqrt(gam_test_MSE)
gam_test_RMSE
```

- Using all of our predictors, our model has an MSE of 2260226 (RMSE 1503.405) when we apply our model to the training data and an MSE of 3012372 (RMSE 1735.619) when we apply it to the testing data from our original partitioning. 

- Alternativelly, the other model has an MSE of 2279817 (RMSE 1509.906) when we apply our model to the training data and an MSE of 3010842 (RMSE 1735.178) when we apply it to the testing data from our original partitioning. 

- In summary, when using all of our predictors, we have effective degrees of freedom, which represent the complexity of the smooth function. `terminal`, `top10perc`, `top25perc`, `personal`, `p_undergrad`, and `enroll` all have df = 1, corresponding to a linear trend. Those with dfs around 2, such as `room_board`` and `books`, suggesting a  quadratic relationship, whereas those with dfs around three, such as `grad_rate` and `accept`, are cubically incorporated, and so forth. 


# c). Train a multivariate adaptive regression spline (MARS) model using all the predictors

```{r}
mars_grid <- expand.grid(degree = 1:2, 
                         nprune = 2:16)
set.seed(1)
mars.fit <- train(x_train, y_train,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)
ggplot(mars.fit)
mars.fit$bestTune
```

*Report the model*

```{r}
# The final MARS model has the following predictors, coefficients, and hinge functions:
coef(mars.fit$finalModel)
```

*Partial dependence plot:*

```{r}
# a single arbitrary predictor in the final model `perc.alumni`:
p1 = pdp::partial(mars.fit, pred.var = c("perc_alumni"), grid.resolution = 10) %>% autoplot()
p1

# interaction partial dependence plot between arbitrary predictors in the final model `Apps` and `Accept`
p2 = pdp::partial(mars.fit, pred.var = c("apps","accept"), grid.resolution = 10) %>% plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 20, x = -60))
p2

grid.arrange(p1, p2, ncol = 2)
```

- The partial dependence plot can be used to visualize and analyze interaction between the target response and a set of input features of interest. Here it shows the marginal effect `perc.alumni`, `Apps`, and `Accept`  features have on the predicted outcome 'Outcome'. 


*Report test error*

```{r}
set.seed(1)

mars.fit_test <- train(x_test, y_test,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

mars.fit_test$results$RMSE
```

```{r}
## Test Error
mars_pred = predict(mars.fit, x_test)
mars_test_rmse = RMSE(mars_pred, y_test)
mars_test_rmse
```

- The final model using MARS is:

- f(x) = 9750.9084463 - 0.74h(expend-15886) - 27.4h(79-grad_rate) + 0.36h(room_board-4323) - 1.05  h(4323-room_board) - 1.57h(1379-f_undergrad) - 91.8h(22-perc_alumni) + 0.44h(apps-3712) + 0.87h(1300-personal) + 0.71h(expend-6897) - 2.03h(enroll-911) + 5.75h(911-enroll) - 1.99h(2109-accept)


- Test error is 1665.72.


# e). Model selection

- Based on the summary results and the plot, MARS model is preferred compared to linear model, since the RMSE of MARS model is smaller than linear model, and the R squared of MARS is a little larger than linear model, which denotes more proportion of y is explained by x.

- Moreover, the test error of MARS (RMSE = 1665.72) is smaller than linear model (RMSE = 1735.178), which further enforces our decision above.

- For general applications, I think MARS is a better approach compared to a linear model. Since MARS models are more flexible than linear regression models. MARS models are simple to understand and interpret.